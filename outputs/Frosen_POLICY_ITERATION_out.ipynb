{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0dea0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T15:44:21.071025Z",
     "iopub.status.busy": "2025-11-09T15:44:21.071025Z",
     "iopub.status.idle": "2025-11-09T15:44:21.086453Z",
     "shell.execute_reply": "2025-11-09T15:44:21.083437Z"
    },
    "papermill": {
     "duration": 0.025527,
     "end_time": "2025-11-09T15:44:21.088497",
     "exception": false,
     "start_time": "2025-11-09T15:44:21.062970",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "rows = 5\n",
    "cols = 5\n",
    "start = \"(0, 0)\"\n",
    "goal = \"(4, 4)\"\n",
    "holes = [\"(0, 3)\", \"(1, 0)\", \"(2, 2)\", \"(3, 0)\", \"(3, 2)\"]\n",
    "slippery = False\n",
    "slip_prob = 0.2\n",
    "gamma = 0.99\n",
    "theta = 1e-08\n",
    "map_desc = [\"SFFHF\", \"HFFFF\", \"FFHFF\", \"HFHFF\", \"FFFFG\"]\n",
    "current_state = 11\n",
    "current_row = 2\n",
    "current_col = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106444546edebfac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T15:44:21.111553Z",
     "iopub.status.busy": "2025-11-09T15:44:21.111553Z",
     "iopub.status.idle": "2025-11-09T15:44:30.243553Z",
     "shell.execute_reply": "2025-11-09T15:44:30.241542Z"
    },
    "papermill": {
     "duration": 9.153088,
     "end_time": "2025-11-09T15:44:30.245555",
     "exception": false,
     "start_time": "2025-11-09T15:44:21.092467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO i7\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start position set to: (0, 0) (index 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode -> reward: 1.0, steps: 8, terminated: True, truncated: False\n",
      "Converged in 9 iterations\n",
      "Optimal V (reshaped if square):\n",
      "[[0.932 0.941 0.951 0.    0.97 ]\n",
      " [0.    0.951 0.961 0.97  0.98 ]\n",
      " [0.951 0.961 0.    0.98  0.99 ]\n",
      " [0.    0.97  0.    0.99  1.   ]\n",
      " [0.97  0.98  0.99  1.    0.   ]]\n",
      "\n",
      "Optimal Policy:\n",
      "→ ↓ ↓ ← ↓\n",
      "← ↓ → ↓ ↓\n",
      "→ ↓ ← ↓ ↓\n",
      "← ↓ ← ↓ ↓\n",
      "→ → → → ←\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters (papermill overrides these when run from the UI)\n",
    "rows = 5\n",
    "cols = 5\n",
    "start = (0, 0)\n",
    "goal = (4, 4)\n",
    "holes = [(0, 3), (1, 0), (2, 2), (3, 0), (3, 2)]\n",
    "slippery = False\n",
    "slip_prob = 0.2\n",
    "gamma = 0.99\n",
    "theta = 1e-8\n",
    "map_desc = None  # optional: UI may pass a list of strings\n",
    "\n",
    "# Policy Iteration on FrozenLake-v1 (parameterized)\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Prefer gymnasium if present\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "import ast\n",
    "\n",
    "# Normalize parameters if papermill injected strings\n",
    "\n",
    "def _to_tuple_2(x):\n",
    "    if isinstance(x, tuple) and len(x) == 2:\n",
    "        return (int(x[0]), int(x[1]))\n",
    "    if isinstance(x, str):\n",
    "        t = ast.literal_eval(x)\n",
    "        return (int(t[0]), int(t[1]))\n",
    "    t = tuple(x)\n",
    "    return (int(t[0]), int(t[1]))\n",
    "\n",
    "\n",
    "def _to_tuple_list(x):\n",
    "    if isinstance(x, str):\n",
    "        v = ast.literal_eval(x)\n",
    "    else:\n",
    "        v = x\n",
    "    return [tuple(map(int, t)) for t in v]\n",
    "\n",
    "start = _to_tuple_2(start)\n",
    "goal = _to_tuple_2(goal)\n",
    "holes = _to_tuple_list(holes)\n",
    "\n",
    "# Build desc only if not provided by UI\n",
    "\n",
    "def build_desc(rows, cols, start, goal, holes):\n",
    "    arr = np.full((rows, cols), 'F', dtype='<U1')\n",
    "    sr, sc = start\n",
    "    gr, gc = goal\n",
    "    arr[sr, sc] = 'S'\n",
    "    arr[gr, gc] = 'G'\n",
    "    for hr, hc in holes:\n",
    "        arr[hr, hc] = 'H'\n",
    "    return [\"\".join(row) for row in arr]\n",
    "\n",
    "if map_desc is None:\n",
    "    map_desc = build_desc(rows, cols, start, goal, holes)\n",
    "\n",
    "# Create env using the provided parameters. Note: slip_prob is not used by gym's FrozenLake.\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=bool(slippery),\n",
    "    desc=map_desc,\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "# Force starting state to provided start coordinates (gym normally starts at 'S')\n",
    "start_index = start[0] * cols + start[1]\n",
    "try:\n",
    "    env.reset()\n",
    "    env.unwrapped.s = start_index  # override internal state\n",
    "except Exception as _e:\n",
    "    pass\n",
    "print(f\"Start position set to: {start} (index {start_index})\")\n",
    "# Access MDP transitions and spaces\n",
    "P = (env.unwrapped.P).copy()\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "# Policy iteration components use the provided gamma/theta\n",
    "\n",
    "def policy_evaluation(pi, V=None, gamma: float = gamma, theta: float = theta):\n",
    "    if V is None:\n",
    "        V = np.zeros(nS, dtype=np.float64)\n",
    "    else:\n",
    "        V = np.array(V, dtype=np.float64, copy=True)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in range(nS):\n",
    "            v_old = V[s]\n",
    "            a = pi[s]\n",
    "            v_new = 0.0\n",
    "            for (prob, ns, r, done) in P[s][a]:\n",
    "                v_new += prob * (r + gamma * (0.0 if done else V[ns]))\n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(V, gamma: float = gamma):\n",
    "    pi = np.zeros(nS, dtype=int)\n",
    "    for s in range(nS):\n",
    "        q = np.zeros(nA, dtype=np.float64)\n",
    "        for a in range(nA):\n",
    "            for (prob, ns, r, done) in P[s][a]:\n",
    "                q[a] += prob * (r + gamma * (0.0 if done else V[ns]))\n",
    "        pi[s] = int(np.argmax(q))\n",
    "    return pi\n",
    "\n",
    "\n",
    "def policy_iteration(gamma: float = gamma, theta: float = theta):\n",
    "    pi = np.random.randint(0, nA, size=nS, dtype=int)\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        V = policy_evaluation(pi, V, gamma=gamma, theta=theta)\n",
    "        new_pi = policy_improvement(V, gamma=gamma)\n",
    "        policy_stable = np.array_equal(pi, new_pi)\n",
    "        pi = new_pi\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return pi, V, iteration\n",
    "\n",
    "\n",
    "pi_opt, V_opt, iters = policy_iteration(gamma=gamma, theta=theta)\n",
    "\n",
    "\n",
    "def run_episode(env, pi):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        a = int(pi[obs])\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "    return total_reward, steps, terminated, truncated\n",
    "\n",
    "\n",
    "total_reward, steps, terminated, truncated = run_episode(env, pi_opt)\n",
    "print(f\"Episode -> reward: {total_reward}, steps: {steps}, terminated: {terminated}, truncated: {truncated}\")\n",
    "\n",
    "print(f\"Converged in {iters} iterations\")\n",
    "side = int(np.sqrt(nS))\n",
    "print(\"Optimal V (reshaped if square):\")\n",
    "if side * side == nS:\n",
    "    print(np.round(V_opt.reshape(side, side), 3))\n",
    "else:\n",
    "    print(np.round(V_opt, 3))\n",
    "\n",
    "arrow_map = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "print(\"\\nOptimal Policy:\")\n",
    "if side * side == nS:\n",
    "    grid = np.array([arrow_map[a] for a in pi_opt]).reshape(side, side)\n",
    "    for r in range(side):\n",
    "        print(\" \".join(grid[r]))\n",
    "else:\n",
    "    print(pi_opt)\n",
    "\n",
    "# Keep window visible briefly and pump events to avoid \"Not Responding\"\n",
    "try:\n",
    "    import pygame\n",
    "    for _ in range(100):  # ~3 seconds\n",
    "        pygame.event.pump()\n",
    "        time.sleep(0.03)\n",
    "except Exception:\n",
    "    time.sleep(3.0)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.169926,
   "end_time": "2025-11-09T15:44:33.317599",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\LENOVO i7\\Desktop\\M2\\Deep Renforcement Learning\\TPs_&_Projects\\frozenlake_rl_app\\game_app\\nootbok\\Frosen_POLICY_ITERATION_to_call.ipynb",
   "output_path": "C:\\Users\\LENOVO i7\\Desktop\\M2\\Deep Renforcement Learning\\TPs_&_Projects\\frozenlake_rl_app\\outputs\\Frosen_POLICY_ITERATION_out.ipynb",
   "parameters": {
    "cols": 5,
    "current_col": 1,
    "current_row": 2,
    "current_state": 11,
    "gamma": 0.99,
    "goal": [
     4,
     4
    ],
    "holes": [
     [
      0,
      3
     ],
     [
      1,
      0
     ],
     [
      2,
      2
     ],
     [
      3,
      0
     ],
     [
      3,
      2
     ]
    ],
    "map_desc": [
     "SFFHF",
     "HFFFF",
     "FFHFF",
     "HFHFF",
     "FFFFG"
    ],
    "rows": 5,
    "slip_prob": 0.2,
    "slippery": false,
    "start": [
     0,
     0
    ],
    "theta": 1e-08
   },
   "start_time": "2025-11-09T15:44:18.147673",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}