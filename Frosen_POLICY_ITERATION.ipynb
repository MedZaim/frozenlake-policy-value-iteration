{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.025276Z",
     "start_time": "2025-11-09T14:50:59.015315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym"
   ],
   "id": "aaebae2aa0de231a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.145703Z",
     "start_time": "2025-11-09T14:50:59.127365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom FrozenLake map (same as policy-iteration notebook)\n",
    "custom_map = [\n",
    "    'SFFHFF',\n",
    "    'HFFFFF',\n",
    "    'FFHFFF',\n",
    "    'HHFHHF',\n",
    "    'FFFGFF'\n",
    "]\n",
    "map_desc = custom_map"
   ],
   "id": "3471c3b41bd20ff",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.226021Z",
     "start_time": "2025-11-09T14:50:59.202231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create env with human rendering so a window appears when stepping\n",
    "env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    is_slippery=False,# True for stochastic environment\n",
    "    desc=map_desc,\n",
    "    render_mode='human'\n",
    ")"
   ],
   "id": "2a761897803ba445",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.292311Z",
     "start_time": "2025-11-09T14:50:59.265937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Access MDP transitions and spaces\n",
    "P = env.unwrapped.P  # dict: P[s][a] -> list of (prob, next_state, reward, terminated)\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "theta = 1e-8\n",
    "\n",
    "\n",
    "print(f\"Number of states: {nS}, Number of actions: {nA}\")\n"
   ],
   "id": "48ce99a081c01d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 30, Number of actions: 4\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.346252Z",
     "start_time": "2025-11-09T14:50:59.338137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# .P → this is a dictionary describing the MDP transitions.\n",
    "# P[s][a] gives a list of possible transitions from state s when taking action a.\n",
    "# Each element in P[s][a] is a tuple: (probability, next_state, reward, terminated)\n"
   ],
   "id": "883395dca271c3e4",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.389905Z",
     "start_time": "2025-11-09T14:50:59.375879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Policy evaluation: given a policy pi (array of action indices), compute V\n",
    "\n",
    "def policy_evaluation(pi, V=None, gamma: float = gamma, theta: float = theta):\n",
    "    if V is None:\n",
    "        V = np.zeros(nS, dtype=np.float64)\n",
    "    else:\n",
    "        V = np.array(V, dtype=np.float64, copy=True)\n",
    "\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in range(nS):\n",
    "            v_old = V[s]\n",
    "            a = pi[s]\n",
    "            v_new = 0.0\n",
    "            for (prob, ns, r, done) in P[s][a]:\n",
    "                v_new += prob * (r + gamma * (0.0 if done else V[ns]))\n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n"
   ],
   "id": "3bb4161ffa3dc567",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.416574Z",
     "start_time": "2025-11-09T14:50:59.405966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Policy improvement: greedy w.r.t. V\n",
    "\n",
    "def policy_improvement(V, gamma: float = gamma):\n",
    "    pi = np.zeros(nS, dtype=int)\n",
    "    for s in range(nS):\n",
    "        q = np.zeros(nA, dtype=np.float64)\n",
    "        for a in range(nA):\n",
    "            for (prob, ns, r, done) in P[s][a]:\n",
    "                q[a] += prob * (r + gamma * (0.0 if done else V[ns]))\n",
    "        pi[s] = int(np.argmax(q))\n",
    "    return pi\n"
   ],
   "id": "4a51ef9d3346b426",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.443521Z",
     "start_time": "2025-11-09T14:50:59.432189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full policy iteration loop\n",
    "\n",
    "def policy_iteration(gamma: float = gamma, theta: float = theta):\n",
    "    # Step 1: Initialize policy randomly and value function\n",
    "    pi = np.random.randint(0, nA, size=nS, dtype=int)\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # Step 2: Evaluate current policy\n",
    "        V = policy_evaluation(pi, V, gamma=gamma, theta=theta)\n",
    "        # Step 3: Improve the policy using the current value function\n",
    "        new_pi = policy_improvement(V, gamma=gamma)\n",
    "        # Step 4: Check if policy has changed\n",
    "        policy_stable = np.array_equal(pi, new_pi)\n",
    "        # Step 5: Update policy\n",
    "        pi = new_pi\n",
    "        # Step 6: Stop if stable (converged)\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return pi, V, iteration\n"
   ],
   "id": "47132f97f827f582",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.467856Z",
     "start_time": "2025-11-09T14:50:59.456336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One episode run to see the window\n",
    "\n",
    "def run_episode(env, pi):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    while not (terminated or truncated):\n",
    "        a = int(pi[obs])\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "    return total_reward, steps, terminated, truncated\n"
   ],
   "id": "e0fe9990cf25c607",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.499674Z",
     "start_time": "2025-11-09T14:50:59.481990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Execute and display results\n",
    "pi_opt, V_opt, iters = policy_iteration(gamma=gamma, theta=theta)\n"
   ],
   "id": "b9939703ba527593",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.540173Z",
     "start_time": "2025-11-09T14:50:59.520531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display optimal values and policy, then run one episode with rendering\n",
    "side = int(np.sqrt(nS))\n",
    "print(f\"Converged in {iters} erations\")\n",
    "\n",
    "# Derive shape\n",
    "rows_map = len(map_desc)\n",
    "cols_map = len(map_desc[0]) if rows_map > 0 else 0\n",
    "\n",
    "print('\\nOptimal V (grid):')\n",
    "if rows_map * cols_map == nS and rows_map > 0:\n",
    "    V_grid = V_opt.reshape(rows_map, cols_map)\n",
    "    for r in range(rows_map):\n",
    "        print(' '.join(f'{V_grid[r, c]:>6.3f}' for c in range(cols_map)))\n",
    "else:\n",
    "    per_row = max(6, int(np.sqrt(nS)))\n",
    "    for i in range(0, nS, per_row):\n",
    "        print(' '.join(f'{v:>6.3f}' for v in V_opt[i:i+per_row]))"
   ],
   "id": "f2066ba2d525269d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 11 erations\n",
      "\n",
      "Optimal V (grid):\n",
      " 0.904  0.914  0.923  0.000  0.941  0.951\n",
      " 0.000  0.923  0.932  0.941  0.951  0.961\n",
      " 0.904  0.914  0.000  0.951  0.961  0.970\n",
      " 0.000  0.000  0.990  0.000  0.000  0.980\n",
      " 0.980  0.990  1.000  0.000  1.000  0.990\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:50:59.636128Z",
     "start_time": "2025-11-09T14:50:59.619913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "arrow_map = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "arrows = [arrow_map[a] for a in pi_opt]\n",
    "if rows_map * cols_map == nS and rows_map > 0:\n",
    "    grid = np.array(arrows).reshape(rows_map, cols_map)\n",
    "    for r in range(rows_map):\n",
    "        print(' '.join(grid[r]))\n",
    "else:\n",
    "    print(' '.join(arrows))\n"
   ],
   "id": "69eee3067e66aaa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "→ ↓ ↓ ← ↓ ↓\n",
      "← → → ↓ ↓ ↓\n",
      "→ ↑ ← → → ↓\n",
      "← ← ↓ ← ← ↓\n",
      "→ → → ← ← ←\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T14:51:07.602107Z",
     "start_time": "2025-11-09T14:50:59.664435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tr, steps, term, trunc = run_episode(env, pi_opt)\n",
    "print(f'\\nEpisode -> reward: {tr}, steps: {steps}, terminated: {term}, truncated: {trunc}')\n",
    "\n",
    "# Keep window visible briefly and pump events to avoid 'Not Responding'\n",
    "try:\n",
    "    import pygame\n",
    "    for _ in range(100):  # ~3 seconds\n",
    "        pygame.event.pump()\n",
    "        time.sleep(0.03)\n",
    "except Exception:\n",
    "    time.sleep(3.0)\n",
    "\n",
    "env.close()"
   ],
   "id": "e56b3cb448da11b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode -> reward: 1.0, steps: 11, terminated: True, truncated: False\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
