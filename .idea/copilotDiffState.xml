<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/_build_value_iteration_notebook.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/_build_value_iteration_notebook.py" />
              <option name="originalContent" value="import nbformat as nbf&#10;from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell&#10;&#10;nb = new_notebook()&#10;&#10;cells = []&#10;&#10;cells.append(new_markdown_cell(&#10;    &quot;# Value Iteration on FrozenLake-v1 (deterministic)\n&quot;&#10;    &quot;This notebook computes the optimal value function and greedy policy using Value Iteration, &quot;&#10;    &quot;on the same custom FrozenLake map (non-slippery), renders one episode, and prints results.&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;import numpy as np\n&quot;&#10;    &quot;import time\n&quot;&#10;    &quot;try:\n&quot;&#10;    &quot;    import gymnasium as gym\n&quot;&#10;    &quot;except ImportError:\n&quot;&#10;    &quot;    import gym\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Custom FrozenLake map (same as policy-iteration notebook)\n&quot;&#10;    &quot;custom_map = [\n&quot;&#10;    &quot;    'SFFHF',\n&quot;&#10;    &quot;    'HFFFF',\n&quot;&#10;    &quot;    'FFHFF',\n&quot;&#10;    &quot;    'HHFHF',\n&quot;&#10;    &quot;    'FFFGF'\n&quot;&#10;    &quot;]\n&quot;&#10;    &quot;map_desc = custom_map\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Create env with human rendering so a window appears when stepping\n&quot;&#10;    &quot;env = gym.make(\n&quot;&#10;    &quot;    'FrozenLake-v1',\n&quot;&#10;    &quot;    is_slippery=False,\n&quot;&#10;    &quot;    desc=map_desc,\n&quot;&#10;    &quot;    render_mode='human'\n&quot;&#10;    &quot;)\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Access MDP transitions and spaces\n&quot;&#10;    &quot;P = (env.unwrapped.P).copy()  # dict: P[s][a] -&gt; list of (prob, next_state, reward, terminated)\n&quot;&#10;    &quot;nS = env.observation_space.n\n&quot;&#10;    &quot;nA = env.action_space.n\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;# Hyperparameters\n&quot;&#10;    &quot;gamma = 0.99\n&quot;&#10;    &quot;theta = 1e-8\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;print(f'Number of states: {nS}, Number of actions: {nA}')\n&quot;&#10;))&#10;&#10;cells.append(new_markdown_cell(&#10;    &quot;P is a dictionary describing the Markov Decision Process (MDP) transitions.\n\n&quot;&#10;    &quot;- P[s][a] gives a list of possible transitions from state s when taking action a.\n&quot;&#10;    &quot;- Each element in P[s][a] is a tuple: (probability, next_state, reward, terminated).&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;def value_iteration(gamma: float = gamma, theta: float = theta):\n&quot;&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;&#10;    &quot;    Compute optimal value function V_* via Bellman optimality updates,\n&quot;&#10;    &quot;    then extract a greedy policy. Returns (pi, V, iterations).\n&quot;&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;&#10;    &quot;    V = np.zeros(nS, dtype=np.float64)\n&quot;&#10;    &quot;    iterations = 0\n&quot;&#10;    &quot;    while True:\n&quot;&#10;    &quot;        delta = 0.0\n&quot;&#10;    &quot;        iterations += 1\n&quot;&#10;    &quot;        for s in range(nS):\n&quot;&#10;    &quot;            v_old = V[s]\n&quot;&#10;    &quot;            q = np.zeros(nA, dtype=np.float64)\n&quot;&#10;    &quot;            for a in range(nA):\n&quot;&#10;    &quot;                for (prob, ns, r, done) in P[s][a]:\n&quot;&#10;    &quot;                    q[a] += prob * (r + gamma * (0.0 if done else V[ns]))\n&quot;&#10;    &quot;            V[s] = np.max(q)\n&quot;&#10;    &quot;            delta = max(delta, abs(v_old - V[s]))\n&quot;&#10;    &quot;        if delta &lt; theta:\n&quot;&#10;    &quot;            break\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;    # Extract greedy policy from the converged value function\n&quot;&#10;    &quot;    pi = np.zeros(nS, dtype=int)\n&quot;&#10;    &quot;    for s in range(nS):\n&quot;&#10;    &quot;        q = np.zeros(nA, dtype=np.float64)\n&quot;&#10;    &quot;        for a in range(nA):\n&quot;&#10;    &quot;            for (prob, ns, r, done) in P[s][a]:\n&quot;&#10;    &quot;                q[a] += prob * (r + gamma * (0.0 if done else V[ns]))\n&quot;&#10;    &quot;        pi[s] = int(np.argmax(q))\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;    return pi, V, iterations\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;pi_opt, V_opt, iters = value_iteration()\n&quot;&#10;    &quot;print(f'Converged in {iters} iterations')\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;def run_episode(env, pi):\n&quot;&#10;    &quot;    obs, info = env.reset()\n&quot;&#10;    &quot;    terminated = False\n&quot;&#10;    &quot;    truncated = False\n&quot;&#10;    &quot;    total_reward = 0.0\n&quot;&#10;    &quot;    steps = 0\n&quot;&#10;    &quot;    while not (terminated or truncated):\n&quot;&#10;    &quot;        a = int(pi[obs])\n&quot;&#10;    &quot;        obs, r, terminated, truncated, info = env.step(a)\n&quot;&#10;    &quot;        total_reward += r\n&quot;&#10;    &quot;        steps += 1\n&quot;&#10;    &quot;    return total_reward, steps, terminated, truncated\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Display optimal values and policy, then run one episode with rendering\n&quot;&#10;    &quot;side = int(np.sqrt(nS))\n&quot;&#10;    &quot;print('Optimal V (reshaped if square):')\n&quot;&#10;    &quot;if side * side == nS:\n&quot;&#10;    &quot;    print(np.round(V_opt.reshape(side, side), 3))\n&quot;&#10;    &quot;else:\n&quot;&#10;    &quot;    print(np.round(V_opt, 3))\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;arrow_map = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n&quot;&#10;    &quot;print('\nOptimal Policy:')\n&quot;&#10;    &quot;if side * side == nS:\n&quot;&#10;    &quot;    grid = np.array([arrow_map[a] for a in pi_opt]).reshape(side, side)\n&quot;&#10;    &quot;    for r in range(side):\n&quot;&#10;    &quot;        print(' '.join(grid[r]))\n&quot;&#10;    &quot;else:\n&quot;&#10;    &quot;    print(pi_opt)\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;total_reward, steps, terminated, truncated = run_episode(env, pi_opt)\n&quot;&#10;    &quot;print(f&quot;\nEpisode -&gt; reward: {total_reward}, steps: {steps}, terminated: {terminated}, truncated: {truncated}&quot;)\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;# Keep window visible briefly and pump events to avoid 'Not Responding'\n&quot;&#10;    &quot;try:\n&quot;&#10;    &quot;    import pygame\n&quot;&#10;    &quot;    for _ in range(100):  # ~3 seconds\n&quot;&#10;    &quot;        pygame.event.pump()\n&quot;&#10;    &quot;        time.sleep(0.03)\n&quot;&#10;    &quot;except Exception:\n&quot;&#10;    &quot;    time.sleep(3.0)\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;env.close()\n&quot;&#10;))&#10;&#10;nb.cells = cells&#10;&#10;nb.metadata[&quot;kernelspec&quot;] = {&#10;    &quot;display_name&quot;: &quot;Python 3 (ipykernel)&quot;,&#10;    &quot;language&quot;: &quot;python&quot;,&#10;    &quot;name&quot;: &quot;python3&quot;,&#10;}&#10;nb.metadata[&quot;language_info&quot;] = {&#10;    &quot;name&quot;: &quot;python&quot;,&#10;    &quot;version&quot;: &quot;3.10.0&quot;,&#10;    &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;    &quot;codemirror_mode&quot;: {&quot;name&quot;: &quot;ipython&quot;, &quot;version&quot;: 3},&#10;    &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;    &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;    &quot;file_extension&quot;: &quot;.py&quot;,&#10;}&#10;&#10;p = r&quot;C:\\Users\\LENOVO i7\\Desktop\\M2\\Deep Renforcement Learning\\TPs_&amp;_Projects\\frozenlake_rl_app\\frosen_VALUE_ITERATION.ipynb&quot;&#10;nbf.write(nb, p)&#10;print(&quot;WROTE:&quot;, p, &quot;cells:&quot;, len(nb.cells))&#10;&#10;" />
              <option name="updatedContent" value="import nbformat as nbf&#10;from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell&#10;&#10;nb = new_notebook()&#10;&#10;cells = []&#10;&#10;cells.append(new_markdown_cell(&#10;    &quot;# Value Iteration on FrozenLake-v1 (deterministic)\n&quot;&#10;    &quot;This notebook computes the optimal value function and greedy policy using Value Iteration, &quot;&#10;    &quot;on the same custom FrozenLake map (non-slippery), renders one episode, and prints results.&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;import numpy as np\n&quot;&#10;    &quot;import time\n&quot;&#10;    &quot;try:\n&quot;&#10;    &quot;    import gymnasium as gym\n&quot;&#10;    &quot;except ImportError:\n&quot;&#10;    &quot;    import gym\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Custom FrozenLake map (same as policy-iteration notebook)\n&quot;&#10;    &quot;custom_map = [\n&quot;&#10;    &quot;    'SFFHF',\n&quot;&#10;    &quot;    'HFFFF',\n&quot;&#10;    &quot;    'FFHFF',\n&quot;&#10;    &quot;    'HHFHF',\n&quot;&#10;    &quot;    'FFFGF'\n&quot;&#10;    &quot;]\n&quot;&#10;    &quot;map_desc = custom_map\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Create env with human rendering so a window appears when stepping\n&quot;&#10;    &quot;env = gym.make(\n&quot;&#10;    &quot;    'FrozenLake-v1',\n&quot;&#10;    &quot;    is_slippery=False,\n&quot;&#10;    &quot;    desc=map_desc,\n&quot;&#10;    &quot;    render_mode='human'\n&quot;&#10;    &quot;)\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Access MDP transitions and spaces\n&quot;&#10;    &quot;P = (env.unwrapped.P).copy()  # dict: P[s][a] -&gt; list of (prob, next_state, reward, terminated)\n&quot;&#10;    &quot;nS = env.observation_space.n\n&quot;&#10;    &quot;nA = env.action_space.n\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;# Hyperparameters\n&quot;&#10;    &quot;gamma = 0.99\n&quot;&#10;    &quot;theta = 1e-8\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;print(f'Number of states: {nS}, Number of actions: {nA}')\n&quot;&#10;))&#10;&#10;cells.append(new_markdown_cell(&#10;    &quot;P is a dictionary describing the Markov Decision Process (MDP) transitions.\n\n&quot;&#10;    &quot;- P[s][a] gives a list of possible transitions from state s when taking action a.\n&quot;&#10;    &quot;- Each element in P[s][a] is a tuple: (probability, next_state, reward, terminated).&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;def value_iteration(gamma: float = gamma, theta: float = theta):\n&quot;&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;&#10;    &quot;    Compute optimal value function V_* via Bellman optimality updates,\n&quot;&#10;    &quot;    then extract a greedy policy. Returns (pi, V, iterations).\n&quot;&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;&#10;    &quot;    V = np.zeros(nS, dtype=np.float64)\n&quot;&#10;    &quot;    iterations = 0\n&quot;&#10;    &quot;    while True:\n&quot;&#10;    &quot;        delta = 0.0\n&quot;&#10;    &quot;        iterations += 1\n&quot;&#10;    &quot;        for s in range(nS):\n&quot;&#10;    &quot;            v_old = V[s]\n&quot;&#10;    &quot;            q = np.zeros(nA, dtype=np.float64)\n&quot;&#10;    &quot;            for a in range(nA):\n&quot;&#10;    &quot;                for (prob, ns, r, done) in P[s][a]:\n&quot;&#10;    &quot;                    q[a] += prob * (r + gamma * (0.0 if done else V[ns]))\n&quot;&#10;    &quot;            V[s] = np.max(q)\n&quot;&#10;    &quot;            delta = max(delta, abs(v_old - V[s]))\n&quot;&#10;    &quot;        if delta &lt; theta:\n&quot;&#10;    &quot;            break\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;    # Extract greedy policy from the converged value function\n&quot;&#10;    &quot;    pi = np.zeros(nS, dtype=int)\n&quot;&#10;    &quot;    for s in range(nS):\n&quot;&#10;    &quot;        q = np.zeros(nA, dtype=np.float64)\n&quot;&#10;    &quot;        for a in range(nA):\n&quot;&#10;    &quot;            for (prob, ns, r, done) in P[s][a]:\n&quot;&#10;    &quot;                q[a] += prob * (r + gamma * (0.0 if done else V[ns]))\n&quot;&#10;    &quot;        pi[s] = int(np.argmax(q))\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;    return pi, V, iterations\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;pi_opt, V_opt, iters = value_iteration()\n&quot;&#10;    &quot;print(f'Converged in {iters} iterations')\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;def run_episode(env, pi):\n&quot;&#10;    &quot;    obs, info = env.reset()\n&quot;&#10;    &quot;    terminated = False\n&quot;&#10;    &quot;    truncated = False\n&quot;&#10;    &quot;    total_reward = 0.0\n&quot;&#10;    &quot;    steps = 0\n&quot;&#10;    &quot;    while not (terminated or truncated):\n&quot;&#10;    &quot;        a = int(pi[obs])\n&quot;&#10;    &quot;        obs, r, terminated, truncated, info = env.step(a)\n&quot;&#10;    &quot;        total_reward += r\n&quot;&#10;    &quot;        steps += 1\n&quot;&#10;    &quot;    return total_reward, steps, terminated, truncated\n&quot;&#10;))&#10;&#10;cells.append(new_code_cell(&#10;    &quot;# Display optimal values and policy, then run one episode with rendering\n&quot;&#10;    &quot;side = int(np.sqrt(nS))\n&quot;&#10;    &quot;print('Optimal V (reshaped if square):')\n&quot;&#10;    &quot;if side * side == nS:\n&quot;&#10;    &quot;    print(np.round(V_opt.reshape(side, side), 3))\n&quot;&#10;    &quot;else:\n&quot;&#10;    &quot;    print(np.round(V_opt, 3))\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;arrow_map = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n&quot;&#10;    &quot;print('\nOptimal Policy:')\n&quot;&#10;    &quot;if side * side == nS:\n&quot;&#10;    &quot;    grid = np.array([arrow_map[a] for a in pi_opt]).reshape(side, side)\n&quot;&#10;    &quot;    for r in range(side):\n&quot;&#10;    &quot;        print(' '.join(grid[r]))\n&quot;&#10;    &quot;else:\n&quot;&#10;    &quot;    print(pi_opt)\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;total_reward, steps, terminated, truncated = run_episode(env, pi_opt)\n&quot;&#10;    &quot;print(f'\\nEpisode -&gt; reward: {total_reward}, steps: {steps}, terminated: {terminated}, truncated: {truncated}')\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;# Keep window visible briefly and pump events to avoid 'Not Responding'\n&quot;&#10;    &quot;try:\n&quot;&#10;    &quot;    import pygame\n&quot;&#10;    &quot;    for _ in range(100):  # ~3 seconds\n&quot;&#10;    &quot;        pygame.event.pump()\n&quot;&#10;    &quot;        time.sleep(0.03)\n&quot;&#10;    &quot;except Exception:\n&quot;&#10;    &quot;    time.sleep(3.0)\n&quot;&#10;    &quot;\n&quot;&#10;    &quot;env.close()\n&quot;&#10;))&#10;&#10;nb.cells = cells&#10;&#10;nb.metadata[&quot;kernelspec&quot;] = {&#10;    &quot;display_name&quot;: &quot;Python 3 (ipykernel)&quot;,&#10;    &quot;language&quot;: &quot;python&quot;,&#10;    &quot;name&quot;: &quot;python3&quot;,&#10;}&#10;nb.metadata[&quot;language_info&quot;] = {&#10;    &quot;name&quot;: &quot;python&quot;,&#10;    &quot;version&quot;: &quot;3.10.0&quot;,&#10;    &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;    &quot;codemirror_mode&quot;: {&quot;name&quot;: &quot;ipython&quot;, &quot;version&quot;: 3},&#10;    &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;    &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;    &quot;file_extension&quot;: &quot;.py&quot;,&#10;}&#10;&#10;p = r&quot;C:\\Users\\LENOVO i7\\Desktop\\M2\\Deep Renforcement Learning\\TPs_&amp;_Projects\\frozenlake_rl_app\\frosen_VALUE_ITERATION.ipynb&quot;&#10;nbf.write(nb, p)&#10;print(&quot;WROTE:&quot;, p, &quot;cells:&quot;, len(nb.cells))" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>